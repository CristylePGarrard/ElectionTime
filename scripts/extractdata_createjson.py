# -*- coding: utf-8 -*-
"""ExtractData_CreateJson.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/143XQ9sUD54eoJceONN5-ko_Ijg5mgUj4
"""

# ####### Install necessary libraries ####### #
!pip install pandas

# ####### Import libraries ####### #
import pandas as pd
import gspread
from gspread_dataframe import get_as_dataframe
from google.colab import auth
from googleapiclient.discovery import build
import os

# ### First we need to get the data we want to use.
# I have a spreadsheet where I've combined the state house
# and the state senators titled 'UTsTateLegIslaTurE_02122025.xlsx'
# Next we need to output the file into a json format so I can use
# javascript to build the webpage.

# ####### Import libraries ####### #


# ####### Mount Google Drive to Colab ####### #
from google.colab import drive
drive.mount('/content/drive')

# ####### Authenticate
auth.authenticate_user()

from google.auth import default
creds, _ = default()
gc = gspread.authorize(creds)
drive_service = build('drive', 'v3', credentials=creds)

# function to loop through google drive folder and make a list of file id's
def get_sheet_urls_from_folder(folder_id):
    query = f"'{folder_id}' in parents and mimeType='application/vnd.google-apps.spreadsheet'"
    results = drive_service.files().list(q=query, fields="files(id, name)").execute()
    files = results.get('files', [])
    return [f"https://docs.google.com/spreadsheets/d/{file['id']}" for file in files]

# Get the URLs of each Google Sheet
sheet_urls_1 = get_sheet_urls_from_folder('1NKiEF5Mq5FD6DSxSJfuXXw7RH_8baZXv')

def read_google_sheet(sheet_url, worksheet_name=None):
    # Open the Google Sheet by its URL
    sheet = gc.open_by_url(sheet_url)
    # Select the worksheet
    if worksheet_name:
        worksheet = sheet.worksheet(worksheet_name)
    else:
        worksheet = sheet.get_worksheet(0)  # Default to the first sheet

    # Convert the worksheet to a DataFrame
    df = get_as_dataframe(worksheet, evaluate_formulas=True).dropna(how='all')  # Drop rows that are completely empty
    df['sheet_name'] = sheet.title  # Add a column with the sheet name
    return df

# There's a weird warning about the format and openpyxl but we don't gotta worry about that so ignore it
import warnings
warnings.simplefilter("ignore")

all_data = []

for url in sheet_urls_1:
    try:
        df = read_google_sheet(url)
        all_data.append(df)
    except Exception as e:
        print(f"Could not read {url}: {e}")

reps = pd.concat(all_data, ignore_index=True)

# Check data
# print(reps.head())
# print(reps.info())

# Clean Data

# Split name into 3 columns
reps[['Last_Name', 'First_Name', 'Middle_Name']] = reps['Representative'].str.split(' ', n=2, expand=True)

# remove comma from the end of the last name
reps['Last_Name'] = reps['Last_Name'].str.rstrip(',')

# remove period from the end of the middle name
reps['Middle_Name'] = reps['Middle_Name'].apply(lambda x: x.rstrip('.') if isinstance(x, str) else x)

# Combine name in correct order
reps['Rep_Name'] = reps['First_Name'] + ' ' + reps['Middle_Name'].fillna('') + ' ' + reps['Last_Name']
reps['Rep_Name'] = reps['Rep_Name'].str.replace(r'\s+', ' ', regex=True)  # Remove multiple spaces

# convert district from float to int
reps['District'] = reps['District'].astype(int)

# convert district from int to string
reps['District'] = reps['District'].astype(str)

# we are going to match reps with the bills that they have passed
# so adding a column Bill Sponsor to make matching easier

reps['Bill Sponsor'] = reps['Rep_Name'].apply(lambda name: f"{name.split()[-1]}, {name[0]}.")

# rearrage columns and remove sheet name from df
reps = reps[['Img_ID', 'Office', 'Rep_Name', 'District',
      'Party', 'Email', 'County(ies)', 'Webpage', 'Img_URL',
       'Legislation_By_Representative', 'Bill Sponsor']].copy().reset_index(drop=True)

# save data as a json file in google drive

print('\n', 'Saving current officials to json file')
file_path = '/content/drive/My Drive/ElectionTime/data/'
save_filename = 'reps.json'
reps.to_json(file_path + save_filename, index=False, orient='records')
print('Script Completed')

# Now that we have the reps info we need to pull in the bills
# I have found two types of data
# on the state leg website there is a downloadable csv with bills that have been passed
# There is also a html table that I copied and pasted to a google sheet with new bills
# Will need to read in data from both so .read_csv and looping through a googledrive folder

# Get the URLs of each Google Sheet
sheet_urls_2 = get_sheet_urls_from_folder('14hEphuFcwv3cfRYSldVVgUjb1ihm8DjO')

all_data = []

for url in sheet_urls_2:
    try:
        df = read_google_sheet(url)
        all_data.append(df)
    except Exception as e:
        print(f"Could not read {url}: {e}")

new_bills = pd.concat(all_data, ignore_index=True)

# Clean Data

# format date numbered column as a data
new_bills['Date Numbered'] = pd.to_datetime(new_bills['Date Numbered'], format='%m/%d/%Y')
# remove the 'Rep' or 'Sen' from the sponsor field
new_bills['Bill Sponsor'] = new_bills['Sponsor'].str.lstrip('Rep. Sen. ')

# format as lastname first initial to match the format of passed_bills dataframe
new_bills['Bill Sponsor'] = new_bills['Bill Sponsor'].str.replace(r'(\w+)-(\w+), (\w)\.', r'\1-\2, \3.', regex=True)

# Read the csv's of the passed bills downloaded from the state legislative website
def read_csv_files(file_path, extension='.csv'):
    files = [x for x in os.listdir(file_path) if x.endswith(extension)]
    all_data = []
    for filename in files:
        try:
            df = pd.read_csv(file_path + filename)
            df['filename'] = filename
            all_data.append(df)
        except Exception as e:
            print(f"Could not read {filename}: {e}")
    return pd.concat(all_data, ignore_index=True)


# ####### Set the file path to uploaded csv file ####### #
file_path = '/content/drive/My Drive/ElectionTime/data/bills/'

# ####### Read the Excel file into a DataFrame ####### #
print("\n", "Reading csv file...")
passed_bills = read_csv_files(file_path)

#check data
passed_bills.head()

# Clean Data

# Extract file date from the filename
passed_bills['file_date'] = [x.split('.')[0].split('_')[1] for x in passed_bills['filename']]
passed_bills['file_date'] = pd.to_datetime(passed_bills['file_date'], format='%m%d%Y')

# Change Date Passed and Effective Date to dates
passed_bills['Date Passed'] = pd.to_datetime(passed_bills['Date Passed'], format='%m/%d/%Y')
passed_bills['Effective Date'] = pd.to_datetime(passed_bills['Effective Date'], format='%m/%d/%Y')

# drop duplicates
print(len(passed_bills), "records BEFORE deduplication")

passed_bills = passed_bills.drop_duplicates(
    subset=[col for col in passed_bills.columns if col not in ['filename', 'file_date']])

# check data
print(len(passed_bills), "records AFTER deduplication")
print(passed_bills['Bill Number'].nunique(), "unique bills vs", len(passed_bills), "total rows")

'''
This dataframe has the info for the bills that have been passed.
I downloaded the data from the state legislative website and
there are multiple files combined into the dataframe
meaning there is likely duplicate data
'''

passed_bills.info()

'''
This dataframe is from a google sheet that I made copying and pasting
n html table from the state legislative website.
These are bills that have been introduced and given a bill number.
'''

new_bills.info()

'''
This dataframe has the information about the representatives.
things like name, emails, district, party and so forth.
I copied and pasted the rosters from the state legislative website
into google sheets that are combined through the python into this dataframe.
it has both the state house and state senate reps.
'''

reps.info()