# -*- coding: utf-8 -*-
"""BadBills_DataPrep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZXNuV1gPbrW2WUoIDOfgzV0EIBlj9vb1
"""

# ####### Install necessary libraries ####### #
!pip install pandas

# ####### Import libraries ####### #
import pandas as pd
import gspread
from gspread_dataframe import get_as_dataframe
from google.colab import auth
from googleapiclient.discovery import build
import os

"""The dataframe called bb is for bad bills. It is a table of state bills that are being worked on this year in the state legislature. It has one line for each bill with things like the description of the bill, who the bill sponsor is, some tags for whether the bill reads as positive, neutral or negative in terms of being for the people. things like that"""

# I have a spreadsheet where i've compiled data from @elevate_pac TikTok account
# The spreadsheet has info about some bills in the utah legislature and their status
# I want to use this info to create timelines showing where the bills are in the process
# ####### Mount Google Drive to Colab ####### #
from google.colab import drive
drive.mount('/content/drive')

# ####### Authenticate
auth.authenticate_user()

from google.auth import default
creds, _ = default()
gc = gspread.authorize(creds)
drive_service = build('drive', 'v3', credentials=creds)

# function to loop through google drive folder and make a list of file id's
def get_sheet_urls_from_folder(folder_id):
    query = f"'{folder_id}' in parents and mimeType='application/vnd.google-apps.spreadsheet'"
    results = drive_service.files().list(q=query, fields="files(id, name)").execute()
    files = results.get('files', [])
    print(f"Found {len(files)} files in the folder:")
    [print(file['name']) for file in files]
    return [f"https://docs.google.com/spreadsheets/d/{file['id']}" for file in files]

def read_google_sheet(sheet_url, worksheet_name=None):
    # Open the Google Sheet by its URL
    sheet = gc.open_by_url(sheet_url)
    # Select the worksheet
    if worksheet_name:
        worksheet = sheet.worksheet(worksheet_name)
    else:
        worksheet = sheet.get_worksheet(0)  # Default to the first sheet

    # Convert the worksheet to a DataFrame
    df = get_as_dataframe(worksheet, evaluate_formulas=True).dropna(how='all')  # Drop rows that are completely empty
    df['sheet_name'] = sheet.title  # Add a column with the sheet name
    return df

# There's a weird warning about the format and openpyxl but we don't gotta worry about that so ignore it
import warnings
warnings.simplefilter("ignore")

# Get the URLs of each Google Sheet
sheet_url = get_sheet_urls_from_folder('1nkEW1IeBOSuSaDkQ4IA27DO23_6AXlU8')

all_data = []

for url in sheet_url:
    try:
        df = read_google_sheet(url)
        all_data.append(df)
    except Exception as e:
        print(f"Could not read {url}: {e}")

bb = pd.concat(all_data, ignore_index=True)

# Check data
# print(bb.head())
print(bb.info())

# Clean up data

bb['Record ID'] = bb['Record ID'].astype(int)  # Convert Record ID to integer

# Remove duplicates
def prioritize_rows(group):
    # If only one row has a Notecard, keep it
    if group['Notecard'].isna().sum() == 1:
        return group.dropna(subset=['Notecard'])
    # If only one row has a Description, keep it
    if group['Description'].isna().sum() == 1:
        return group.dropna(subset=['Description'])
    # Otherwise, keep the first occurrence
    return group.iloc[:1]

# Group by 'Bill Number' and 'Bill Title' and apply our prioritization
bb = bb.groupby(['Bill Number', 'Bill Title'], group_keys=False).apply(prioritize_rows)

# Reset index
bb.reset_index(drop=True, inplace=True)

# Fill missing values where appropriate
bb['Description'].fillna('No description available', inplace=True)
bb['Topics'].fillna('Other', inplace=True)
bb['Read'].fillna('Not specified', inplace=True)

# Ensure all text columns are strings
text_columns = ['Bill Number', 'Bill Title', 'Description', 'Notecard', 'Topics', 'Read', 'Bill Sponsor', 'sheet_name']
bb[text_columns] = bb[text_columns].astype(str)

# Trim whitespace from all string columns
bb = bb.applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Replace empty strings with NaN for consistency
bb.replace("", pd.NA, inplace=True)

# Create a column called 'Office' and use 'Bill Sponsor' to populate the value
bb['Office'] = bb['Bill Sponsor'].apply(lambda x: 'State House' if str(x).startswith('Rep.') else
                                                  'State Senate' if str(x).startswith('Sen.') else pd.NA)
# Remove the prefix from the 'Bill Sponsor'
bb['Bill Sponsor'] = bb['Bill Sponsor'].apply(lambda x: x.replace('Rep.', '').replace('Sen.', '').strip())

# Display results
print(bb[['Bill Sponsor', 'Office']].head())

# Display cleaned dataframe info
print(bb.info())
# print(bb.head())

bb['Office'].value_counts()

bb.loc[bb['Record ID'] == 99]

bb.loc[bb['Bill Number']=='TEST RECORD']

# DROP TEST RECORD, JUST THERE TO CHECK FOR DUPLICATES AND SUCH
bb = bb[bb['Bill Number'] != 'TEST RECORD'].copy().reset_index(drop=True)

"""This new dataframe called updts is from a google sheet that has the bill number and the 'Notecard' from the bb data but then it has columns for the status of the bill (Process Tag), the date the status changed (Date) and what day of the legislature that it is (Day of Legislature). I want to use this date to create a timeline of the bills."""

# import bad bills daily update file for the daily updates to the bills
sheet_url = get_sheet_urls_from_folder('1vD6e4XnGPSQsy0qm79TKIeRsUe0luewR')

updates = read_google_sheet(sheet_url[0])

# Check data
# print(updates.head())
print(updates.info())

# Clean up the data

# Convert Record ID to int
updates['Record_ID'] = updates['Record_ID'].astype(int)

# Remove extra whitespace in text fields
updates['Bill Number'] = updates['Bill Number'].str.strip()
updates['Notecard'] = updates['Notecard'].str.strip()
updates['Process Tag'] = updates['Process Tag'].str.strip()

# Convert Date to datetime format
updates['Date'] = pd.to_datetime(updates['Date'], errors='coerce')

# Convert Day of Legislature to int, handling NaN values
updates['Day of Legislature'] = pd.to_numeric(updates['Day of Legislature'], errors='coerce').fillna(0).astype(int)

# Drop duplicates if necessary
updates.drop_duplicates(subset=['Bill Number', 'Notecard', 'Process Tag', 'Date'], inplace=True)

# Fill missing values if needed (adjust based on context)
updates.fillna({'Process Tag': 'Unknown', 'Day of Legislature': 0}, inplace=True)

# Print summary to verify changes
print(updates.info())
# print(updates.head())

''' Now for the process tags there is a specified order. A typical bill will follow this order
It starts in the house that submitted it so if it's a house bill (prefix HB) it starts in the house
and if it's a senate bill (prefix SB) it starts in the senate.
The bill will go through rules, committee and than 3 votes on the floor then repeat those same steps
for the other house or senate. The bill then goes to the governor to sign and then the bill will have
officially passed. If when the bill is in the second house and they make changes to the bill it will go back
to the original house for concurrance before going to the governor. Any bill can go to the graveyard at any
point in the process. This doesn't alway mean it was voted down but could just be held in committee
or something. but based off how they tipically operate it will likely be discarded and never go any further
but a bill can come back from the graveyard and continue in the process from where it left off.
'''

from pandas.api.types import CategoricalDtype

# This ensures the process tags maintain a defined order so when building the
# timeline it always follows this order even if some steps are skipped
process_order = [
    "Rules 1", "Committee 1", "Floor Vote 1.1", "Floor Vote 1.2", "Floor Vote 1.3",
    "Rules 2", "Committee 2", "Floor Vote 2.1", "Floor Vote 2.2", "Floor Vote 2.3",
    "Governor", "Bill Passed", "Concurrence", "Graveyard", "Vetoed"
]

process_cat = CategoricalDtype(categories=process_order, ordered=True)
updates['Process Tag'] = updates['Process Tag'].astype(process_cat)

# The graveyard tag is a special situation because bills can move in an out
# so we need a way to track the sequence of changes for each bill

updates = updates.sort_values(by=['Bill Number', 'Date', 'Process Tag'])

# Now we can merge the bb and the updates dataframes
# I'll use the Bill Number field for the merge because this should be an exact match in both dfs

merged_df = updates.merge(bb, on="Bill Number", how="left", suffixes=("_updates", "_bb"))

# check for missing bills
missing_bills = merged_df[merged_df['Bill Title'].isna()]
print("Bills in updates but not in bb:\n", missing_bills[['Bill Number']])



bb.loc[bb['Record ID'] == 99]

# Import reps_bills_combined.json file that ExtractData_CreateJson script creates

# Combine bb and reps dataframes to create json file